{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook serves to train and benchmark models on a dataset annotated by label-studio \nThis notebook is made to work on GPU, created on kaggle","metadata":{}},{"cell_type":"code","source":"!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'","metadata":{"execution":{"iopub.status.busy":"2023-10-17T08:17:27.249984Z","iopub.execute_input":"2023-10-17T08:17:27.250344Z","iopub.status.idle":"2023-10-17T08:20:27.223323Z","shell.execute_reply.started":"2023-10-17T08:17:27.250317Z","shell.execute_reply":"2023-10-17T08:20:27.222192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch, detectron2\n!nvcc --version\nTORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\nCUDA_VERSION = torch.__version__.split(\"+\")[-1]\nprint(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\nprint(\"detectron2:\", detectron2.__version__)","metadata":{"id":"2OrF5nJyGEC1","outputId":"9246c5c7-c69a-43b5-fb25-e254b61fbf0e","execution":{"iopub.status.busy":"2023-10-17T08:20:27.225582Z","iopub.execute_input":"2023-10-17T08:20:27.226254Z","iopub.status.idle":"2023-10-17T08:20:28.383021Z","shell.execute_reply.started":"2023-10-17T08:20:27.226218Z","shell.execute_reply":"2023-10-17T08:20:28.381768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from detectron2.config import get_cfg\nfrom detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_train_loader\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.model_zoo import model_zoo\nfrom detectron2.utils.visualizer import Visualizer, ColorMode\nfrom detectron2.structures import BoxMode\nfrom io import BytesIO\nimport os\nimport cv2\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport PIL.Image\nimport requests\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader","metadata":{"id":"RsSWYnl7H5tn","execution":{"iopub.status.busy":"2023-10-17T08:20:28.384939Z","iopub.execute_input":"2023-10-17T08:20:28.385537Z","iopub.status.idle":"2023-10-17T08:20:28.996828Z","shell.execute_reply.started":"2023-10-17T08:20:28.3855Z","shell.execute_reply":"2023-10-17T08:20:28.99596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the following cellule you have to choose the path for your dataset.  \nthe dataset has to contain a .json with the annotations and an \"image\" folder with all the images named after their id.","metadata":{}},{"cell_type":"code","source":"#MappingLabelStudio créer une instance à partir du fichier .json générer par LabelStudio\ndataset_path = \"/kaggle/input/galica\"\n\njson_file = dataset_path + \"/galica/gallicaimages_set1.json\"\nimage_dir =  dataset_path + \"/image\"\n\n#list des categories sur lesquels on souhaite travailler\ncategory_dict = {\n  'tampon': 0,\n  'écriture manuscrite': 1,\n  'écriture typographique': 2,\n  'photographie': 3,\n  'estampe': 4,\n  'décoration' : 5\n}","metadata":{"id":"hwwlYricPwLg","execution":{"iopub.status.busy":"2023-10-17T08:22:37.026609Z","iopub.execute_input":"2023-10-17T08:22:37.02699Z","iopub.status.idle":"2023-10-17T08:22:37.031613Z","shell.execute_reply.started":"2023-10-17T08:22:37.026962Z","shell.execute_reply":"2023-10-17T08:22:37.030685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation :  this section will do all the premilimnary work to adapt the data for the training","metadata":{}},{"cell_type":"code","source":"def update_image_sizes(json_file, image_dir, out):\n    with open(json_file, encoding='utf-8') as f:\n        data = json.load(f)\n\n    for image in data['images']:\n        image_id = image['id']\n        image_path = image_dir + '/' + image_id + '.jpg'  # Chemin complet de l'image\n\n        # Ouvrir l'image avec PIL\n        img = PIL.Image.open(image_path)\n        width, height = img.size\n\n        # Mettre à jour les informations de taille dans le JSON\n        image['width'] = width\n        image['height'] = height\n\n    # Écrire le JSON mis à jour dans un nouveau fichier\n    with open(out, 'w') as outfile:\n        json.dump(data, outfile)\n\n    print(\"Mise à jour des tailles d'image terminée.\")\n    print(\"JSON mis à jour enregistré dans :\", \"gallica_dataset_file.json\")\n    \nupdate_image_sizes(json_file, image_dir, \"gallica_dataset_file.json\")","metadata":{"id":"6F5figVFkqRB","execution":{"iopub.status.busy":"2023-10-17T08:20:29.01906Z","iopub.execute_input":"2023-10-17T08:20:29.019799Z","iopub.status.idle":"2023-10-17T08:20:29.027956Z","shell.execute_reply.started":"2023-10-17T08:20:29.01977Z","shell.execute_reply":"2023-10-17T08:20:29.027166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gallica_json = \"/kaggle/working/gallica_dataset_file_all.json\"","metadata":{"execution":{"iopub.status.busy":"2023-10-17T08:20:29.044315Z","iopub.execute_input":"2023-10-17T08:20:29.045181Z","iopub.status.idle":"2023-10-17T08:20:29.052976Z","shell.execute_reply.started":"2023-10-17T08:20:29.045152Z","shell.execute_reply":"2023-10-17T08:20:29.052097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\ndef get_my_dataset_dicts(json_file, category_dict, image_dir):\n    \"\"\"\n        from gallica Label Studio to COCO dataset json\n    \"\"\"\n    with open(json_file, encoding='utf-8') as f:\n        data = json.load(f)\n\n    dataset_dicts = []\n    for idx, image in enumerate(data['images']):\n        record = {}\n        # Create file path from local directory and image id\n        filename = os.path.join(image_dir, image['id'] + '.jpg')\n\n        # Assign image properties\n        record[\"file_name\"] = filename\n        record[\"height\"] = image[\"height\"] \n        record[\"width\"] = image[\"width\"]\n        record[\"image_id\"] = image['id']\n\n        annotations = []\n        # Assign annotations to image\n        for ann in data[\"annotations\"]:\n            for result in ann[\"result\"]:\n                if ann[\"id\"] == image[\"id\"] and type(result[\"label\"]) is not int and result[\"label\"][0] in category_dict.keys():\n                # Create a new dict for each annotation in the image\n                    x = int(result['bbox']['x'] / 100.0 * image[\"width\"])\n                    y = int(result['bbox']['y'] / 100.0 * image[\"height\"])\n                    width = int(result['bbox']['width'] / 100.0 * image[\"width\"])\n                    height = int(result['bbox']['height'] / 100.0 * image[\"height\"])\n                    obj = {\n                        \"bbox\": [x, y, width, height],\n                        \"bbox_mode\": BoxMode.XYWH_ABS,  # as your bounding box coordinates are in absolute format\n                        \"category_id\": category_dict[result[\"label\"][0]],  # map your label name to its corresponding id\n#                         \"id\": result[\"id\"],\n#                         \"iscrowd\": 0,\n#                         \"image_id\":ann[\"id\"],\n                    }\n                    annotations.append(obj)\n        record[\"annotations\"] = annotations\n        dataset_dicts.append(record)\n    return dataset_dicts","metadata":{"id":"Svs4f5lyxtgY","execution":{"iopub.status.busy":"2023-10-17T08:22:55.017686Z","iopub.execute_input":"2023-10-17T08:22:55.018089Z","iopub.status.idle":"2023-10-17T08:22:55.027007Z","shell.execute_reply.started":"2023-10-17T08:22:55.018061Z","shell.execute_reply":"2023-10-17T08:22:55.026189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef remove_all_datasets():\n    registered_datasets = list(DatasetCatalog.list())\n    for dataset_name in registered_datasets:\n        DatasetCatalog.remove(dataset_name)\nremove_all_datasets()","metadata":{"id":"c9tghaVcbAUX","execution":{"iopub.status.busy":"2023-10-17T08:22:55.781607Z","iopub.execute_input":"2023-10-17T08:22:55.782268Z","iopub.status.idle":"2023-10-17T08:22:55.786694Z","shell.execute_reply.started":"2023-10-17T08:22:55.782237Z","shell.execute_reply":"2023-10-17T08:22:55.785838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PRINT DATASET\n\ndef print_coco_dataset(dataset):\n    my_dataset_metadata = MetadataCatalog.get(dataset)\n\n    # Get your dataset in Detectron2's format\n    dataset_dicts = DatasetCatalog.get(dataset)\n\n    for d in random.sample(dataset_dicts, 3):\n        # Open the image file\n        img = PIL.Image.open(d[\"file_name\"])\n        img = np.array(img)\n\n        # Handle grayscale images:\n        if len(img.shape) == 2:\n            img = np.stack([img] * 3, axis=-1)\n\n        # Create a visualizer instance\n        visualizer = Visualizer(img[:, :, ::-1], metadata=my_dataset_metadata, scale=0.5)\n\n        # Draw the predictions on the image\n        vis = visualizer.draw_dataset_dict(d)\n\n        # Show the image using matplotlib\n        plt.imshow(vis.get_image()[:, :, ::-1])\n        plt.axis('off')\n        plt.show()\n","metadata":{"id":"cjMfeFbSC2vn","outputId":"87bf1262-9039-473c-e646-80c3d844adf0","execution":{"iopub.status.busy":"2023-10-17T08:22:57.211784Z","iopub.execute_input":"2023-10-17T08:22:57.212155Z","iopub.status.idle":"2023-10-17T08:22:57.218918Z","shell.execute_reply.started":"2023-10-17T08:22:57.212129Z","shell.execute_reply":"2023-10-17T08:22:57.217881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_data(json_file, category_dict, image_dir, split='train'):\n    # Load json file\n    \"\"\"\n        Divide original dataset in train , val , test\n    \"\"\"\n    with open(json_file) as f:\n        data = json.load(f)\n\n    classes_count = np.zeros(len(category_dict))\n    # Convert to Detectron2 format\n    dataset_dicts = get_my_dataset_dicts(json_file, category_dict, image_dir)\n    print(len(dataset_dicts))\n    \n    #get total number of each classes\n    for data in dataset_dicts:        \n        for annotation in data[\"annotations\"]:\n            classes_count[annotation[\"category_id\"]] += 1\n    \n    data_train = []\n    data_val = []\n    data_test = []\n    data_lost = []\n    nb_image = 0\n    count = np.zeros(len(category_dict))\n    classes_count_sorted = np.argsort(classes_count)\n    for data in dataset_dicts:\n        isLost = True\n        for annotation in data[\"annotations\"]:\n            v = False\n            for i in range(len(category_dict)):\n                if count[annotation[\"category_id\"]] < classes_count[classes_count_sorted[i]]*0.83:\n                    data_train.append(data)\n                    for annotation in data[\"annotations\"]:\n                        count[annotation[\"category_id\"]] += 1\n                    nb_image+=1\n                    v=True\n                    break\n                elif count[annotation[\"category_id\"]] < classes_count[classes_count_sorted[i]]*0.95:\n                    data_val.append(data)\n                    for annotation in data[\"annotations\"]:\n                        count[annotation[\"category_id\"]] += 1\n                    nb_image+=1\n                    v=True\n                    break\n                elif count[annotation[\"category_id\"]] < classes_count[classes_count_sorted[i]]:\n                    data_test.append(data)\n                    for annotation in data[\"annotations\"]:\n                        count[annotation[\"category_id\"]] += 1\n                    nb_image+=1\n                    v=True\n                    break\n            if v:\n                isLost = False\n                break\n        if isLost:\n            data_lost.append(data)\n            \n    #append lost data to test data\n    for data in data_lost:\n        data_test.append(data)\n                        \n    print(\"count: \", count)\n    print(\"classes_count: \", classes_count)\n    print(\"nb_image: \", nb_image)\n    print(\"data_lost\", len(data_lost))\n    \n    return data_train, data_val, data_test\n    \ndatasets = split_data(gallica_json, category_dict, image_dir)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T08:35:46.693164Z","iopub.execute_input":"2023-10-17T08:35:46.693493Z","iopub.status.idle":"2023-10-17T08:35:51.232287Z","shell.execute_reply.started":"2023-10-17T08:35:46.693468Z","shell.execute_reply":"2023-10-17T08:35:51.231386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This cellule will give you a .rar containing all the test images so you can choose visually some cases that you would try, we will see later how to try these images.","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\ntest_images = []\nfor i in range(len(datasets[2])):\n    test_images.append(datasets[2][i][\"image_id\"])\n\n\n# Define source and destination directories\nimage_dir = dataset_path + \"/image\"  # Your source directory\ndestination_dir = \"/kaggle/working/test_images\"  # Replace with your destination directory\n\n# Create the destination directory if it doesn't exist\nif not os.path.exists(destination_dir):\n    os.makedirs(destination_dir)\n\n# Iterate through the list of image filenames and copy them to the destination directory\nfor image_filename in test_images:\n    # Add the .jpg extension to the filenames\n    image_filename_with_extension = image_filename + \".jpg\"\n    \n    source_path = os.path.join(image_dir, image_filename_with_extension)\n    destination_path = os.path.join(destination_dir, image_filename_with_extension)\n    \n    try:\n        shutil.copy(source_path, destination_path)\n        print(f\"Successfully copied {image_filename} to {destination_dir}\")\n    except FileNotFoundError:\n        print(f\"File not found: {image_filename}\")\n    except FileExistsError:\n        print(f\"File already exists in the destination directory: {image_filename}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-17T10:05:18.493503Z","iopub.execute_input":"2023-10-17T10:05:18.49389Z","iopub.status.idle":"2023-10-17T10:05:19.358615Z","shell.execute_reply.started":"2023-10-17T10:05:18.493862Z","shell.execute_reply":"2023-10-17T10:05:19.357573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip_file_name = \"images.zip\"\n\n# Zip the destination directory\nshutil.make_archive(destination_dir, 'zip', destination_dir)\n\n# Rename the generated archive to the desired name\nos.rename(destination_dir + '.zip', zip_file_name)\nprint(f\"Successfully zipped the directory as {zip_file_name}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-17T10:07:40.022549Z","iopub.execute_input":"2023-10-17T10:07:40.022961Z","iopub.status.idle":"2023-10-17T10:07:47.355023Z","shell.execute_reply.started":"2023-10-17T10:07:40.022932Z","shell.execute_reply":"2023-10-17T10:07:47.354026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataset_dicts_split(datasets, split='train'):\n    # Load json file\n    \"\"\"\n        Divide original dataset in train , val , test\n    \"\"\"\n    \n    # Shuffle data\n    random.shuffle(datasets[0])\n    random.shuffle(datasets[1])\n    random.shuffle(datasets[2])\n\n    if split == 'train':\n        return datasets[0]\n    elif split == 'val':\n        return datasets[1]\n    elif split == 'test':\n        return datasets[2]","metadata":{"id":"s7dtpc1yM8ZH","execution":{"iopub.status.busy":"2023-10-17T08:36:22.767753Z","iopub.execute_input":"2023-10-17T08:36:22.768123Z","iopub.status.idle":"2023-10-17T08:36:22.773868Z","shell.execute_reply.started":"2023-10-17T08:36:22.768099Z","shell.execute_reply":"2023-10-17T08:36:22.772884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remove_all_datasets()\n\nfor d in [\"train\", \"val\", \"test\"]:\n    DatasetCatalog.register(\"my_dataset_\" + d, lambda d=d: get_dataset_dicts_split(datasets, d))\n    MetadataCatalog.get(\"my_dataset_\" + d).set(thing_classes=list(category_dict.keys()))","metadata":{"id":"PvatQwEjM_pv","execution":{"iopub.status.busy":"2023-10-17T08:36:31.183751Z","iopub.execute_input":"2023-10-17T08:36:31.184141Z","iopub.status.idle":"2023-10-17T08:36:31.18999Z","shell.execute_reply.started":"2023-10-17T08:36:31.184113Z","shell.execute_reply":"2023-10-17T08:36:31.188543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data visualization  \nDisplaying a few images from the validation set","metadata":{}},{"cell_type":"code","source":"print_coco_dataset(\"my_dataset_val\")","metadata":{"execution":{"iopub.status.busy":"2023-10-17T08:36:31.739749Z","iopub.execute_input":"2023-10-17T08:36:31.740123Z","iopub.status.idle":"2023-10-17T08:36:34.354579Z","shell.execute_reply.started":"2023-10-17T08:36:31.740097Z","shell.execute_reply":"2023-10-17T08:36:34.353808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This is the training section with default values  \nIn the below cellule you can modify with your own training parameters","metadata":{}},{"cell_type":"code","source":"from detectron2.engine import DefaultTrainer\nfrom detectron2.config import get_cfg","metadata":{"id":"oPEULXunNXTu","execution":{"iopub.status.busy":"2023-10-17T08:36:49.539595Z","iopub.execute_input":"2023-10-17T08:36:49.53996Z","iopub.status.idle":"2023-10-17T08:36:49.544966Z","shell.execute_reply.started":"2023-10-17T08:36:49.539932Z","shell.execute_reply":"2023-10-17T08:36:49.54386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = get_cfg()\n\n\n# Specify the model to use\ncfg.merge_from_file(\"/kaggle/input/modeldata/config.yml\")\n\n# Override the dataset and solver settings\ncfg.DATASETS.TRAIN = (\"my_dataset_train\",)\ncfg.DATASETS.TEST = (\"my_dataset_val\",)\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.WEIGHTS = \"/kaggle/input/modeldata/model_final.pth\"\ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR = 0.0025\ncfg.SOLVER.MAX_ITER = 1000 \ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = len(category_dict)","metadata":{"id":"g89tPNyKNZes","execution":{"iopub.status.busy":"2023-10-17T08:36:49.894721Z","iopub.execute_input":"2023-10-17T08:36:49.895088Z","iopub.status.idle":"2023-10-17T08:36:49.935409Z","shell.execute_reply.started":"2023-10-17T08:36:49.895061Z","shell.execute_reply":"2023-10-17T08:36:49.934567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = DefaultTrainer(cfg)\ntrainer.resume_or_load(resume=False)\n\n# Measure the training time\nstart_time = time.time()\ntrainer.train()\nend_time = time.time()\n\n# Calculate the elapsed time\nelapsed_time = end_time - start_time\n\nprint(\"Training time: {:.4f} seconds\".format(elapsed_time))","metadata":{"id":"k5lDusLrNfZ0","outputId":"f27922df-fe8d-4be3-820e-71badee44770","execution":{"iopub.status.busy":"2023-10-17T08:37:02.45406Z","iopub.execute_input":"2023-10-17T08:37:02.454403Z","iopub.status.idle":"2023-10-17T08:45:43.357429Z","shell.execute_reply.started":"2023-10-17T08:37:02.454377Z","shell.execute_reply":"2023-10-17T08:45:43.355949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from detectron2.evaluation import COCOEvaluator, inference_on_dataset, COCOPanopticEvaluator\nfrom detectron2.data import build_detection_test_loader\nfrom detectron2.data import MetadataCatalog\n\n# Get the metadata for your dataset\nmy_dataset_metadata = MetadataCatalog.get(\"my_dataset_val\")\n\n# Create an evaluator for the validation set\nevaluator = COCOEvaluator(\"my_dataset_val\", output_dir=\"./output\")\n\n# Create a test data loader\ntest_loader = build_detection_test_loader(cfg, \"my_dataset_val\")\n\n# Perform inference on the validation set\ntry:\n    results = inference_on_dataset(trainer.model, test_loader, evaluator)\nexcept Exception as e:\n    # Print the error message\n    print(\"Error:\", str(e))","metadata":{"execution":{"iopub.status.busy":"2023-10-17T08:45:43.360043Z","iopub.execute_input":"2023-10-17T08:45:43.360952Z","iopub.status.idle":"2023-10-17T08:48:50.598989Z","shell.execute_reply.started":"2023-10-17T08:45:43.360927Z","shell.execute_reply":"2023-10-17T08:48:50.59792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from detectron2.utils.visualizer import ColorMode\nfrom detectron2.engine import DefaultPredictor\n\n\n#Use the final weights generated after successful training for inference  \ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8  # set the testing threshold for this model\n#Pass the validation dataset\ncfg.DATASETS.TEST = (\"my_dataset_val\", )\n\npredictor = DefaultPredictor(cfg)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T08:49:02.766019Z","iopub.execute_input":"2023-10-17T08:49:02.766352Z","iopub.status.idle":"2023-10-17T08:49:03.785514Z","shell.execute_reply.started":"2023-10-17T08:49:02.766325Z","shell.execute_reply":"2023-10-17T08:49:03.784541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from detectron2.checkpoint import DetectionCheckpointer, Checkpointer\n\ncheckpointer = DetectionCheckpointer(trainer.model, save_dir=cfg.OUTPUT_DIR)\ncheckpointer.save(\"my_model\")","metadata":{"execution":{"iopub.status.busy":"2023-10-17T08:49:03.787312Z","iopub.execute_input":"2023-10-17T08:49:03.788199Z","iopub.status.idle":"2023-10-17T08:49:04.314401Z","shell.execute_reply.started":"2023-10-17T08:49:03.788165Z","shell.execute_reply":"2023-10-17T08:49:04.313154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This section allow you to visualize the performance of your model on images  \nin this cellule you can add the file names of the images selected earlier in the .rar to see how the model predict on them","metadata":{}},{"cell_type":"code","source":"dataset_dicts = DatasetCatalog.get(\"my_dataset_test\")\nmetadata = MetadataCatalog.get(\"my_dataset_test\")\nfile_names = [\"5f84a46e-a568-462f-8476-35866b41d2ff\",\n\"6a6925ac-5ad8-4829-a88f-0109293e3cd3\",\n\"13a8a57e-3017-497f-a3f3-0dd15abdf70f\",\n\"67a07b78-f7cd-4768-9b12-b65dbc4a396c\",\n\"a5aab339-5d30-405a-bc58-c396bc46002d\"]\n\ndataset_test = []\nfor data in dataset_dicts:\n    if data[\"image_id\"] in file_names:\n        dataset_test.append(data)\n\nfor d in dataset_test:\n    im = cv2.imread(d[\"file_name\"])\n\n    # Make predictions\n    outputs = predictor(im)\n\n    # Visualize both ground truth and predictions\n    visualizer_pred = Visualizer(\n        im[:, :, ::-1], metadata=metadata, scale=0.8, instance_mode=ColorMode.IMAGE\n    )\n    visualizer_gt = Visualizer(\n        im[:, :, ::-1], metadata=metadata, scale=0.8, instance_mode=ColorMode.IMAGE\n    )\n\n\n    # Draw ground truth bounding boxes\n    gt_visualized = visualizer_gt.draw_dataset_dict(d)\n\n    # Draw instance predictions\n    predictions_visualized = visualizer_pred.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n\n\n    # Combine the images horizontally\n    final_image = np.concatenate((gt_visualized.get_image()[:, :, ::-1], predictions_visualized.get_image()[:, :, ::-1]), axis=1)\n    \n    # Assuming you have final_image generated\n    final_image_bgr = final_image[:, :, ::-1]  # Convert to BGR format\n\n    if not os.path.exists(\"/kaggle/working/predictions\"):\n        os.makedirs(\"/kaggle/working/predictions\")\n    \n    # Save the image to a file (e.g., final_image.png)\n    cv2.imwrite('/kaggle/working/predictions/'+d[\"image_id\"]+'.png', final_image_bgr)\n\n    # Show the combined image using matplotlib\n    plt.imshow(final_image)\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T10:39:49.492775Z","iopub.execute_input":"2023-10-17T10:39:49.493798Z","iopub.status.idle":"2023-10-17T10:40:08.034806Z","shell.execute_reply.started":"2023-10-17T10:39:49.493749Z","shell.execute_reply":"2023-10-17T10:40:08.033962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip_file_name = \"predictions.zip\"\n\n# Zip the destination directory\nshutil.make_archive(\"/kaggle/working/predictions/\", 'zip', \"/kaggle/working/predictions/\")\n\n# Rename the generated archive to the desired name\nos.rename(\"/kaggle/working/predictions/\" + '.zip', zip_file_name)\nprint(f\"Successfully zipped the directory as {zip_file_name}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-17T10:41:40.24479Z","iopub.execute_input":"2023-10-17T10:41:40.245205Z","iopub.status.idle":"2023-10-17T10:43:18.691481Z","shell.execute_reply.started":"2023-10-17T10:41:40.245178Z","shell.execute_reply":"2023-10-17T10:43:18.688109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This section is the evaluation of the model performances with coco panoptic","metadata":{}},{"cell_type":"markdown","source":"COCO Panoptic Metric Explanation\n\nThe COCO Panoptic Metric is a widely used evaluation measure in computer vision, specifically designed to assess the performance of algorithms and models in the context of panoptic segmentation. Panoptic segmentation is a task that unifies the understanding of both stuff (e.g., road, sky) and things (e.g., cars, people) in a scene, making it a comprehensive way to analyze and interpret images and videos.\n\nThe COCO Panoptic Metric takes into account the quality of segmentation, ensuring that objects are correctly delineated, and the understanding of the overall scene context, providing a holistic assessment of the model's performance in scene understanding.\n\nThe evaluation is typically performed by comparing the model's output to ground truth annotations. The metric is expressed as a combination of the average \"thing\" and \"stuff\" segmentation accuracies, along with the \"panoptic quality,\" which summarizes the model's ability to capture the complete scene.","metadata":{}},{"cell_type":"code","source":"def predict_directory(path):\n    n = 0\n    for file_name in os.listdir(path):\n        if n >= 5:\n            break\n        # Read the image\n        file_path = os.path.join(path, file_name)\n        im = cv2.imread(file_path)\n\n        # Measure the prediction time\n        start_time = time.time()\n        outputs = predictor(im)\n        end_time = time.time()\n        \n        # Calculate the elapsed time\n        elapsed_time = end_time - start_time\n\n        print(\"Inference time: {:.4f} seconds, shape of {}\".format(elapsed_time, im.shape))\n        \n        # Extract the instance masks and their associated classes\n        instances = outputs[\"instances\"].to(\"cpu\")\n        #instance_masks = instances.pred_masks.numpy()\n        print(instances.pred_boxes.tensor.tolist())\n        print(instances.pred_classes.tolist())\n\n        # Visualize the predictions\n        v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.8, instance_mode=ColorMode.IMAGE)\n        v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n\n        # Display the image\n        plt.imshow(v.get_image()[:, :, ::-1])\n        plt.axis('off')\n        plt.show()\n        n+=1\n        \npredict_directory(\"/kaggle/input/dataset2/dataset2/1418\")","metadata":{"execution":{"iopub.status.busy":"2023-10-17T08:49:10.513055Z","iopub.execute_input":"2023-10-17T08:49:10.513738Z","iopub.status.idle":"2023-10-17T08:49:20.958174Z","shell.execute_reply.started":"2023-10-17T08:49:10.513703Z","shell.execute_reply":"2023-10-17T08:49:20.957362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m pip install coco-pano-ext-demo","metadata":{"execution":{"iopub.status.busy":"2023-10-17T08:49:20.959394Z","iopub.execute_input":"2023-10-17T08:49:20.960167Z","iopub.status.idle":"2023-10-17T08:49:29.854919Z","shell.execute_reply.started":"2023-10-17T08:49:20.960134Z","shell.execute_reply":"2023-10-17T08:49:29.853702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_size = (400,400)\n\ndef create_mask_with_json(data, width, height):\n    masks = [np.zeros(img_size, dtype=np.uint8) for x in range(len(category_dict))]\n    #mask = np.zeros(img_size, dtype=np.uint8)\n    for annotation in data[\"annotations\"]:\n        # Convert annotation units to pixel values\n        x = int(annotation['bbox'][0])\n        y = int(annotation['bbox'][1])\n        x2 = int(annotation['bbox'][2])\n        y2 = int(annotation['bbox'][3])\n        \n        # Scale the coordinates to match the mask size\n        x_scaled = int(x * img_size[1] / width)      # New x-coordinate after scaling\n        y_scaled = int(y * img_size[0] / height)     # New y-coordinate after scaling\n        width_scaled = int(x2 * img_size[1] / width)    # New width after scaling\n        height_scaled = int(y2 * img_size[0] / height)  # New height after scaling\n        \n\n        # Assign the corresponding label value to the pixels within the scaled bounding box\n        masks[annotation[\"category_id\"]][y_scaled:y_scaled+height_scaled, x_scaled:x_scaled+width_scaled] = 1\n        \n    return np.array(masks)\n\ndef create_mask_with_prediction(boxes, classes, width, height):\n    masks = [np.zeros(img_size, dtype=np.uint8) for x in range(len(category_dict))]\n    for i in range(len(boxes)):\n        # Convert annotation units to pixel values\n        x = int(boxes[i][0])\n        y = int(boxes[i][1])\n        x2 = int(boxes[i][2])\n        y2 = int(boxes[i][3])\n\n        # Scale the coordinates to match the mask size\n        x_scaled = int(x * img_size[1] / width)      # New x-coordinate after scaling\n        y_scaled = int(y * img_size[0] / height)     # New y-coordinate after scaling\n        width_scaled = int(x2 * img_size[1] / width)    # New width after scaling\n        height_scaled = int(y2 * img_size[0] / height)  # New height after scaling\n\n        # Assign the corresponding label value to the pixels within the scaled bounding box\n        masks[classes[i]][y_scaled:height_scaled, x_scaled:width_scaled] = 1\n    \n    return np.array(masks)\n\ndef predict_image(path):\n    # Read the image\n    im = cv2.imread(path)\n\n    # Perform prediction\n    outputs = predictor(im)\n\n    # Extract the instance masks and their associated classes\n    instances = outputs[\"instances\"].to(\"cpu\")\n\n    return (instances.pred_boxes.tensor.tolist(),instances.pred_classes.tolist()) \n        \n    \ngroundtruth = []\npredictions = []\nfor data in dataset_dicts:\n    groundtruth.append(create_mask_with_json(data, data[\"width\"], data[\"height\"]))\n    prediction = predict_image(data[\"file_name\"])\n    predictions.append(create_mask_with_prediction(prediction[0], prediction[1], data[\"width\"], data[\"height\"]))","metadata":{"execution":{"iopub.status.busy":"2023-10-17T08:49:29.85824Z","iopub.execute_input":"2023-10-17T08:49:29.858624Z","iopub.status.idle":"2023-10-17T08:50:30.225626Z","shell.execute_reply.started":"2023-10-17T08:49:29.858587Z","shell.execute_reply":"2023-10-17T08:50:30.224696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef show_pred(n, class_predicted=1):\n    truth = []\n    for annotation in dataset_dicts[n][\"annotations\"]:\n        l = annotation['bbox'].copy()\n        l[2] += l[0]\n        l[3] += l[1]\n        truth.append(l)\n    print(\"groundtruth: \", truth)\n    # Read the image\n    im = cv2.imread(dataset_dicts[n][\"file_name\"])\n\n    # Perform prediction\n    outputs = predictor(im)\n\n    # Extract the instance masks and their associated classes\n    instances = outputs[\"instances\"].to(\"cpu\")\n    #instance_masks = instances.pred_masks.numpy()\n    print(\"prediction: \", instances.pred_boxes.tensor.tolist())\n\n    # Visualize the predictions\n    v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.8, instance_mode=ColorMode.IMAGE)\n    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n\n    # Display the image\n    plt.imshow(v.get_image()[:, :, ::-1])\n    plt.axis('off')\n    plt.show()\n    \n    plt.imshow(groundtruth[n][class_predicted], cmap='binary')  # 'binary' colormap for black and white\n    plt.title('Binary Map')\n    #plt.colorbar()  # Add a colorbar to show the values\n    plt.show()\n\n    plt.imshow(predictions[n][class_predicted], cmap='binary')  # 'binary' colormap for black and white\n    plt.title('Binary Map')\n    #plt.colorbar()  # Add a colorbar to show the values\n    plt.show()\n\nshow_pred(0, 2)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T08:50:30.523769Z","iopub.execute_input":"2023-10-17T08:50:30.524603Z","iopub.status.idle":"2023-10-17T08:50:32.232035Z","shell.execute_reply.started":"2023-10-17T08:50:30.524555Z","shell.execute_reply":"2023-10-17T08:50:32.231168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from coco_pano_ext_demo import COCO_plot, COCO\nfrom coco_pano_ext_demo.coco import _compute_labelmap, _compute_iou\nfrom coco_pano_ext_demo.iou import compute_matching_scores","metadata":{"execution":{"iopub.status.busy":"2023-10-17T09:13:29.509322Z","iopub.execute_input":"2023-10-17T09:13:29.509669Z","iopub.status.idle":"2023-10-17T09:13:29.513988Z","shell.execute_reply.started":"2023-10-17T09:13:29.509642Z","shell.execute_reply":"2023-10-17T09:13:29.513032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_matching_weights(target_binary_image, pred_binary_image) -> tuple[list[float], list[float]]:\n    # extract connected components\n    T = _compute_labelmap(target_binary_image)\n    P = _compute_labelmap(pred_binary_image)\n    # computes IoUs\n    wtp, wpt = 0,0#_compute_iou(T, P)\n    if np.all(pred_binary_image==0):\n        wtp, wpt = _compute_iou(T, T)\n    elif np.all(target_binary_image==0):\n        wtp, wpt = _compute_iou(P, P)\n    else:\n        wtp, wpt = _compute_iou(T, P)\n    # remove background components\n    wtp, wpt = wtp[1:], wpt[1:]\n    return wtp, wpt","metadata":{"execution":{"iopub.status.busy":"2023-10-17T09:51:54.431495Z","iopub.execute_input":"2023-10-17T09:51:54.4322Z","iopub.status.idle":"2023-10-17T09:51:54.438121Z","shell.execute_reply.started":"2023-10-17T09:51:54.432173Z","shell.execute_reply":"2023-10-17T09:51:54.436864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Iterable\nimport pandas as pd\ndef compute_pq_score_list_single_class(targets: Iterable[np.ndarray], predictions: Iterable[np.ndarray]) -> tuple[float, float, float, pd.DataFrame]:\n    # Init global accumulators\n    W_TtoP_global = []\n    W_PtoT_global = []\n    # loop over predictions (single class, TODO repeat for each class)\n    for T0, P0 in zip(targets, predictions):\n        # Compute pairwise matching scores, exluding background\n        wtp, wpt = compute_matching_weights(T0, P0)\n        # Update global matching lists\n        W_TtoP_global.extend(wtp.tolist())\n        W_PtoT_global.extend(wpt.tolist())\n    # report final score\n    pairing_threshold = 0.5\n    df = compute_matching_scores(np.array(W_TtoP_global), np.array(W_PtoT_global), pairing_threshold)\n    COCO_SQ = df[\"IoU\"].mean() if len(df) > 0 else 0\n    COCO_RQ = df[\"F-score\"].iloc[0] if len(df) > 0 else 0\n    COCO_PQ = COCO_SQ * COCO_RQ\n    \n    return COCO_PQ, COCO_RQ, COCO_SQ, df","metadata":{"execution":{"iopub.status.busy":"2023-10-17T09:51:54.809319Z","iopub.execute_input":"2023-10-17T09:51:54.809661Z","iopub.status.idle":"2023-10-17T09:51:54.816777Z","shell.execute_reply.started":"2023-10-17T09:51:54.809634Z","shell.execute_reply":"2023-10-17T09:51:54.815949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get the average iou of the predictions\nscores_per_class = np.zeros(len(category_dict))\n\nfor i in range(len(category_dict)):\n    T = np.array(groundtruth)[:,i]\n    P = np.array(predictions)[:,i]\n    scores_per_class[i], COCO_RQ, COCO_SQ, df = compute_pq_score_list_single_class(T, P)\n\nscores_per_class","metadata":{"execution":{"iopub.status.busy":"2023-10-17T09:51:55.158161Z","iopub.execute_input":"2023-10-17T09:51:55.159229Z","iopub.status.idle":"2023-10-17T09:51:58.343678Z","shell.execute_reply.started":"2023-10-17T09:51:55.159189Z","shell.execute_reply":"2023-10-17T09:51:58.342375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Create a DataFrame\ndf = pd.DataFrame({'Class': list(category_dict.keys()), 'Precision': scores_per_class})\n\n# Remove rows with NaN precision scores\ndf = df.dropna(subset=['Precision'])\n\n# Create a histogram\nplt.bar(df['Class'], df['Precision'])\nplt.xlabel('Class')\nplt.ylabel('Precision')\nplt.title('Precision per Class')\nplt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n\n# Show the plot\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-16T10:05:37.153513Z","iopub.status.idle":"2023-10-16T10:05:37.154155Z","shell.execute_reply.started":"2023-10-16T10:05:37.153929Z","shell.execute_reply":"2023-10-16T10:05:37.153952Z"},"trusted":true},"execution_count":null,"outputs":[]}]}